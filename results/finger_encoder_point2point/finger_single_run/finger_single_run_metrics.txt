Training_loss: [0.33808234 0.29799265 0.29325396 0.2905786  0.2889266  0.28788894
 0.28711355 0.28665289 0.28631502 0.28609264 0.28583458 0.28570089
 0.28558549 0.28545722 0.2853629  0.28527758 0.28514561 0.28499961
 0.28507331 0.28490457 0.28478897 0.28475371 0.28475162 0.28462464
 0.2846325  0.28454635 0.28453371 0.28441983 0.28435218 0.28421658
 0.28418493 0.28412643 0.28410619 0.28421015 0.28400916 0.28391105
 0.28385946 0.28376967 0.2836712  0.28365251 0.28338882 0.28314185
 0.28260767 0.28184289 0.28130445 0.28090474 0.28039905 0.28010732
 0.27952492 0.27900705 0.27830416 0.27733856 0.27601579 0.2748487
 0.2724387  0.27065161 0.26826417 0.26534784 0.26137057 0.2567533
 0.25318286 0.24881203 0.24394514 0.23814601 0.23253132 0.2283038
 0.22283278 0.21887013 0.21362394 0.20860873 0.20528698 0.20189382
 0.19877364 0.19521433 0.19363941 0.19122037 0.18890214 0.18692087
 0.18422945 0.18310018 0.18046594 0.17953905 0.17723644 0.17647098
 0.17533673 0.17344294 0.17252609 0.17114216 0.17007563 0.16967866
 0.16938303 0.16785185 0.16659759 0.16592737 0.16507937 0.16433685
 0.16393775 0.16316119 0.16235842 0.16211866 0.16135642 0.16053306
 0.15978113 0.15901384 0.1589912  0.15835407 0.15795183 0.15715487
 0.1574558  0.15520881 0.15551621 0.15458278 0.15471138 0.15455239
 0.15345936 0.15378302 0.15296932 0.15323773 0.15228671 0.15143302
 0.15173356 0.15172765 0.15034516 0.15142745 0.15050232 0.15092461
 0.14681193 0.14639917 0.14658859 0.14638536 0.14657563 0.14570796
 0.14561778 0.14465924 0.1442879  0.14385056 0.14399573 0.14420687
 0.14363135 0.14340268 0.14367349 0.14353679 0.1435094  0.14359938
 0.143564   0.14328335 0.14333607 0.14334066 0.14324743 0.14285718
 0.14280759 0.14210975 0.14185819 0.14215271 0.14194722 0.14179055
 0.14187996 0.14164509 0.14184269 0.14133008 0.14143789 0.14121234
 0.14122064]
Validation_loss: [0.2909925  0.28861916 0.2880429  0.2879034  0.28798175 0.28770414
 0.28761306 0.2875382  0.28803527 0.2876004  0.28760141 0.28764442
 0.28744665 0.28774601 0.28785589 0.28728274 0.28722528 0.28709891
 0.2871784  0.28695229 0.28706661 0.28682843 0.28679296 0.28684667
 0.28664979 0.28654838 0.28645414 0.28692594 0.28630787 0.28646341
 0.28649732 0.28619057 0.28649864 0.28630584 0.28603762 0.28610909
 0.28592065 0.28587577 0.28566572 0.28560933 0.28529876 0.28477311
 0.28397241 0.28370887 0.28300101 0.28270608 0.28223211 0.28185284
 0.28151077 0.28062078 0.27966911 0.27813533 0.27613923 0.27653804
 0.2723037  0.27018815 0.26694873 0.26462317 0.26017639 0.25739065
 0.25160384 0.24950004 0.24321218 0.23682564 0.23187587 0.2281699
 0.22121648 0.21443333 0.21102521 0.2101523  0.20441127 0.20248118
 0.20015779 0.19635426 0.19149566 0.18982045 0.18616216 0.18939772
 0.18607847 0.18553036 0.18117163 0.1834963  0.18173669 0.17955138
 0.17711255 0.17635943 0.17382847 0.17400068 0.17235234 0.17547099
 0.17500691 0.17177281 0.17122561 0.16810848 0.16720772 0.16840816
 0.16761836 0.16577162 0.17195037 0.16440611 0.16482356 0.16640002
 0.16490158 0.16263743 0.16410491 0.1630996  0.1661206  0.16618976
 0.1631494  0.16102119 0.16374706 0.16241439 0.16196761 0.15990287
 0.16040935 0.15944971 0.15771803 0.15856378 0.15896073 0.15577511
 0.15974179 0.16077149 0.15852852 0.16008982 0.16106145 0.15657626
 0.15474842 0.15544666 0.15648806 0.15605296 0.1569905  0.15570155
 0.15615985 0.1558045  0.15511385 0.15570165 0.15440659 0.15455724
 0.15424952 0.1561923  0.15620379 0.15479603 0.15342516 0.15579455
 0.15315378 0.15436324 0.15718435 0.15448022 0.15533057 0.15430611
 0.15358894 0.15402484 0.1528714  0.15358286 0.15405942 0.1542463
 0.15420131 0.1545352  0.15412529 0.15324081 0.15325029 0.15384381
 0.15352976]
Test_loss: [0.1685342]
Epochs: [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.
  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.
  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.
  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.
  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.
 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.
 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.
 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.
 155. 156. 157. 158. 159. 160. 161. 162. 163.]
Euclidean Distance: 83.07898712158203
DTW Distance: 0.1260969961153098
Pearson Correlation: 0.820783812650567
Spearman Correlation: 0.7343912960159539
MSE: 0.02840377576649189
MAE: 0.12606537342071533
RMSE: 0.16853420436382294
NRMSE: 0.09240304678678513
NMAE: 0.06911846250295639
Parameters: {'subject': [10], 'action': 'run', 'sequence_length': 1000, 'sequence_step_size': 100, 'subset': 1, 'd_model': 144, 'input_dim': 3, 'output_dim': 1, 'nhead': 6, 'num_layers': 4, 'batch_size': 16, 'dropout': 0.1, 'num_epochs': 300, 'learning_rate': 0.001, 'ppg_scaling_factor': 100}
General: {'model_name': 'finger_single_run', 'model_type': 'encoder', 'dataset': 'Finger', 'normalization': 'subject_wise', 'train': True, 'eval': True, 'use_dataloader': True, 'train_shuffling': True, 'filter': 1, 'validation_set_subjects': 1, 'test_set_subjects': 1, 'random_seed': 17}
Output: {'model_family': 'finger_encoder_point2point/', 'results': '../results/', 'checkpoints': '../models/checkpoints/', 'model_summary': '../models/model_summary/'}
