{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "import torch\n",
    "import pickle\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Add the parent directory (i.e. transformer, means parent directory of 'scripts' and 'notebooks') to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import the function\n",
    "from scripts.utils import *\n",
    "from scripts.classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Evaluating model with general settings: {'model_name': 'm1_autoregressive_decoding_server', 'train': True, 'eval': True, 'use_dataloader': True, 'train_shuffling': True, 'filter': 1, 'validation_set_subjects': 3, 'test_set_subjects': 3, 'random_seed': 42} \n",
      "\n",
      "Evaluating model with parameters: {'subject': '_all', 'action': 'all', 'sequence_length': 1000, 'sequence_step_size': 50, 'subset': 0.001, 'd_model': 6, 'input_dim': 3, 'output_dim': 1, 'nhead': 3, 'num_layers': 2, 'batch_size': 4, 'num_epochs': 300, 'learning_rate': 0.001, 'ppg_scaling_factor': 100} \n",
      "\n",
      "Saving files to folders: ../results/m1_ablation/m1_autoregressive_decoding_server\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with open(\"config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "    \n",
    "model_name = config['general']['model_name']\n",
    "model_family = config['output']['model_family']\n",
    "model_summary_folder = config['output']['model_summary'] + model_family + model_name\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = select_device()\n",
    "\n",
    "# Load data\n",
    "with open(f\"{model_summary_folder}/{model_name}_saved_metadata.pkl\", \"rb\") as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Convert loaded data to device\n",
    "loaded_data = load_data_to_device(loaded_data, device)\n",
    "\n",
    " # Retrieve data\n",
    "loaded_config = loaded_data[\"Configuration_file\"]\n",
    "scalers = loaded_data[\"Scalers\"] # Dictionary\n",
    "train_subjects = loaded_data[\"Training subjects\"]\n",
    "val_subjects = loaded_data[\"Validation subjects\"]\n",
    "test_subjects = loaded_data[\"Test subjects\"]\n",
    "# X_train = loaded_data[\"X_train\"]  # PyTorch tensor\n",
    "# y_train = loaded_data[\"y_train\"]  # PyTorch tensor\n",
    "# X_val = loaded_data[\"X_val\"]      # PyTorch tensor\n",
    "# y_val = loaded_data[\"y_val\"]      # PyTorch tensor\n",
    "X_test = loaded_data[\"X_test\"]      # PyTorch tensor\n",
    "y_test = loaded_data[\"y_test\"]      # PyTorch tensor\n",
    "# df_train = loaded_data[\"df_train\"]  # Pandas DataFrame\n",
    "# df_val = loaded_data[\"df_val\"]      # Pandas DataFrame\n",
    "df_test = loaded_data[\"df_test\"] # Pandas DataFrame\n",
    "training_loss = loaded_data[\"Training_loss\"]  # Numpy array\n",
    "validation_loss = loaded_data[\"Validation_loss\"]  # Numpy array\n",
    "epochs = loaded_data[\"Epochs\"]  # Numpy array\n",
    "\n",
    "\n",
    "assert loaded_config['general']['model_name'] == model_name\n",
    "general = loaded_config['general']\n",
    "params = loaded_config['parameters']\n",
    "assert loaded_config['output']['model_family'] == model_family \n",
    "results_folder = loaded_config['output']['results'] + model_family + model_name \n",
    "d_model = loaded_config['parameters']['d_model']   # Embedding dimension\n",
    "input_dim = loaded_config['parameters']['input_dim']   # 3 PPG signals (red, green, IR)\n",
    "output_dim = loaded_config['parameters']['output_dim']  # 1 ECG target per time step\n",
    "nhead = loaded_config['parameters']['nhead']   # Attention heads\n",
    "num_layers = loaded_config['parameters']['num_layers']   # Number of transformer layers\n",
    "batch_size = loaded_config['parameters']['batch_size']   # Batch size\n",
    "sequence_length = loaded_config['parameters']['sequence_length'] \n",
    "num_epochs = loaded_config['parameters']['num_epochs']  \n",
    "use_dataloader = loaded_config['general']['use_dataloader'] # Use a dataloader or not\n",
    "random_seed = loaded_config['general']['random_seed']\n",
    "\n",
    "print(f\"Evaluating model with general settings: {general}\", '\\n')\n",
    "print(f\"Evaluating model with parameters: {params}\", '\\n')\n",
    "print(f\"Saving files to folders: {results_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ft/8d1p935913j1_tk94srvt2lm0000gn/T/ipykernel_6447/3935230863.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"../models/{model_family}{model_name}_trained_model.pth\", map_location=torch.device(device)))\n"
     ]
    }
   ],
   "source": [
    "# Use a random seed for reproducibility \n",
    "seed = config['general']['random_seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed) \n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = TransformerTimeSeries(input_dim=input_dim, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers).to(device) \n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load(f\"../models/{model_family}{model_name}_trained_model.pth\", map_location=torch.device(device)))\n",
    "\n",
    "# Select cpu, gpu or mps device for evaluation\n",
    "device = select_device()\n",
    "\n",
    "### Validation\n",
    "# Initialize storage for aggregated predictions and actual values\n",
    "ecg_predictions = []\n",
    "ecg_actuals = []\n",
    "ppg = []\n",
    "subjects = []  # Store subject info for each batch\n",
    "actions = []   # Store action info for each batch\n",
    "\n",
    "# Loss function: Mean Squared Error for regression tasks\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Test Loss\n",
    "test_loss = np.array([])\n",
    "running_test_loss = 0\n",
    "\n",
    "# Check if the dataloader should be used or not\n",
    "if use_dataloader:\n",
    "    # Convert tensors to Datasets\n",
    "    test_dataset = PreprocessedDataset(X_test, y_test)\n",
    "    # Create DataLoaders for each set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([4, 1000, 1])\n",
      "torch.Size([1, 1000, 1])\n",
      "tensor([[[-0.3442],\n",
      "         [-0.3420],\n",
      "         [-0.3397],\n",
      "         [-0.3375],\n",
      "         [-0.3353],\n",
      "         [-0.3331],\n",
      "         [-0.3310],\n",
      "         [-0.3289],\n",
      "         [-0.3269],\n",
      "         [-0.3249],\n",
      "         [-0.3230],\n",
      "         [-0.3212],\n",
      "         [-0.3195],\n",
      "         [-0.3178],\n",
      "         [-0.3162],\n",
      "         [-0.3147],\n",
      "         [-0.3133],\n",
      "         [-0.3120],\n",
      "         [-0.3108],\n",
      "         [-0.3096],\n",
      "         [-0.3085],\n",
      "         [-0.3074],\n",
      "         [-0.3064],\n",
      "         [-0.3055],\n",
      "         [-0.3045],\n",
      "         [-0.3036],\n",
      "         [-0.3026],\n",
      "         [-0.3017],\n",
      "         [-0.3007],\n",
      "         [-0.2996],\n",
      "         [-0.2985],\n",
      "         [-0.2972],\n",
      "         [-0.2958],\n",
      "         [-0.2943],\n",
      "         [-0.2926],\n",
      "         [-0.2908],\n",
      "         [-0.2887],\n",
      "         [-0.2864],\n",
      "         [-0.2839],\n",
      "         [-0.2810],\n",
      "         [-0.2779],\n",
      "         [-0.2745],\n",
      "         [-0.2708],\n",
      "         [-0.2668],\n",
      "         [-0.2625],\n",
      "         [-0.2578],\n",
      "         [-0.2528],\n",
      "         [-0.2474],\n",
      "         [-0.2418],\n",
      "         [-0.2358],\n",
      "         [-0.2296],\n",
      "         [-0.2231],\n",
      "         [-0.2163],\n",
      "         [-0.2094],\n",
      "         [-0.2022],\n",
      "         [-0.1950],\n",
      "         [-0.1876],\n",
      "         [-0.1802],\n",
      "         [-0.1727],\n",
      "         [-0.1653],\n",
      "         [-0.1580],\n",
      "         [-0.1509],\n",
      "         [-0.1439],\n",
      "         [-0.1372],\n",
      "         [-0.1309],\n",
      "         [-0.1248],\n",
      "         [-0.1193],\n",
      "         [-0.1141],\n",
      "         [-0.1095],\n",
      "         [-0.1055],\n",
      "         [-0.1020],\n",
      "         [-0.0992],\n",
      "         [-0.0970],\n",
      "         [-0.0955],\n",
      "         [-0.0948],\n",
      "         [-0.0947],\n",
      "         [-0.0953],\n",
      "         [-0.0966],\n",
      "         [-0.0986],\n",
      "         [-0.1013],\n",
      "         [-0.1046],\n",
      "         [-0.1084],\n",
      "         [-0.1127],\n",
      "         [-0.1175],\n",
      "         [-0.1227],\n",
      "         [-0.1282],\n",
      "         [-0.1339],\n",
      "         [-0.1397],\n",
      "         [-0.1455],\n",
      "         [-0.1512],\n",
      "         [-0.1567],\n",
      "         [-0.1619],\n",
      "         [-0.1666],\n",
      "         [-0.1708],\n",
      "         [-0.1742],\n",
      "         [-0.1768],\n",
      "         [-0.1785],\n",
      "         [-0.1791],\n",
      "         [-0.1786],\n",
      "         [-0.1768],\n",
      "         [-0.1737],\n",
      "         [-0.1691],\n",
      "         [-0.1630],\n",
      "         [-0.1553],\n",
      "         [-0.1461],\n",
      "         [-0.1353],\n",
      "         [-0.1228],\n",
      "         [-0.1087],\n",
      "         [-0.0931],\n",
      "         [-0.0761],\n",
      "         [-0.0576],\n",
      "         [-0.0378],\n",
      "         [-0.0168],\n",
      "         [ 0.0051],\n",
      "         [ 0.0279],\n",
      "         [ 0.0514],\n",
      "         [ 0.0753],\n",
      "         [ 0.0994],\n",
      "         [ 0.1235],\n",
      "         [ 0.1474],\n",
      "         [ 0.1707],\n",
      "         [ 0.1932],\n",
      "         [ 0.2147],\n",
      "         [ 0.2348],\n",
      "         [ 0.2535],\n",
      "         [ 0.2703],\n",
      "         [ 0.2852],\n",
      "         [ 0.2979],\n",
      "         [ 0.3083],\n",
      "         [ 0.3162],\n",
      "         [ 0.3215],\n",
      "         [ 0.3241],\n",
      "         [ 0.3240],\n",
      "         [ 0.3213],\n",
      "         [ 0.3158],\n",
      "         [ 0.3078],\n",
      "         [ 0.2973],\n",
      "         [ 0.2845],\n",
      "         [ 0.2695],\n",
      "         [ 0.2526],\n",
      "         [ 0.2339],\n",
      "         [ 0.2137],\n",
      "         [ 0.1922],\n",
      "         [ 0.1698],\n",
      "         [ 0.1466],\n",
      "         [ 0.1229],\n",
      "         [ 0.0991],\n",
      "         [ 0.0752],\n",
      "         [ 0.0516],\n",
      "         [ 0.0286],\n",
      "         [ 0.0062],\n",
      "         [-0.0153],\n",
      "         [-0.0357],\n",
      "         [-0.0550],\n",
      "         [-0.0729],\n",
      "         [-0.0894],\n",
      "         [-0.1043],\n",
      "         [-0.1178],\n",
      "         [-0.1296],\n",
      "         [-0.1399],\n",
      "         [-0.1486],\n",
      "         [-0.1557],\n",
      "         [-0.1614],\n",
      "         [-0.1656],\n",
      "         [-0.1684],\n",
      "         [-0.1699],\n",
      "         [-0.1702],\n",
      "         [-0.1695],\n",
      "         [-0.1677],\n",
      "         [-0.1651],\n",
      "         [-0.1617],\n",
      "         [-0.1577],\n",
      "         [-0.1531],\n",
      "         [-0.1482],\n",
      "         [-0.1429],\n",
      "         [-0.1374],\n",
      "         [-0.1318],\n",
      "         [-0.1262],\n",
      "         [-0.1207],\n",
      "         [-0.1153],\n",
      "         [-0.1101],\n",
      "         [-0.1052],\n",
      "         [-0.1006],\n",
      "         [-0.0964],\n",
      "         [-0.0926],\n",
      "         [-0.0892],\n",
      "         [-0.0862],\n",
      "         [-0.0837],\n",
      "         [-0.0817],\n",
      "         [-0.0801],\n",
      "         [-0.0790],\n",
      "         [-0.0783],\n",
      "         [-0.0780],\n",
      "         [-0.0782],\n",
      "         [-0.0786],\n",
      "         [-0.0794],\n",
      "         [-0.0805],\n",
      "         [-0.0818],\n",
      "         [-0.0834],\n",
      "         [-0.0851],\n",
      "         [-0.0869],\n",
      "         [-0.0889],\n",
      "         [-0.0908],\n",
      "         [-0.0928],\n",
      "         [-0.0947],\n",
      "         [-0.0966],\n",
      "         [-0.0983],\n",
      "         [-0.0998],\n",
      "         [-0.1012],\n",
      "         [-0.1023],\n",
      "         [-0.1031],\n",
      "         [-0.1036],\n",
      "         [-0.1037],\n",
      "         [-0.1035],\n",
      "         [-0.1029],\n",
      "         [-0.1018],\n",
      "         [-0.1003],\n",
      "         [-0.0983],\n",
      "         [-0.0959],\n",
      "         [-0.0929],\n",
      "         [-0.0893],\n",
      "         [-0.0853],\n",
      "         [-0.0807],\n",
      "         [-0.0755],\n",
      "         [-0.0698],\n",
      "         [-0.0636],\n",
      "         [-0.0568],\n",
      "         [-0.0495],\n",
      "         [-0.0417],\n",
      "         [-0.0335],\n",
      "         [-0.0247],\n",
      "         [-0.0155],\n",
      "         [-0.0060],\n",
      "         [ 0.0040],\n",
      "         [ 0.0142],\n",
      "         [ 0.0247],\n",
      "         [ 0.0355],\n",
      "         [ 0.0464],\n",
      "         [ 0.0574],\n",
      "         [ 0.0685],\n",
      "         [ 0.0796],\n",
      "         [ 0.0906],\n",
      "         [ 0.1015],\n",
      "         [ 0.1122],\n",
      "         [ 0.1226],\n",
      "         [ 0.1327],\n",
      "         [ 0.1424],\n",
      "         [ 0.1516],\n",
      "         [ 0.1603],\n",
      "         [ 0.1685],\n",
      "         [ 0.1760],\n",
      "         [ 0.1828],\n",
      "         [ 0.1889],\n",
      "         [ 0.1943],\n",
      "         [ 0.1988],\n",
      "         [ 0.2026],\n",
      "         [ 0.2055],\n",
      "         [ 0.2076],\n",
      "         [ 0.2088],\n",
      "         [ 0.2092],\n",
      "         [ 0.2089],\n",
      "         [ 0.2077],\n",
      "         [ 0.2057],\n",
      "         [ 0.2031],\n",
      "         [ 0.1998],\n",
      "         [ 0.1960],\n",
      "         [ 0.1915],\n",
      "         [ 0.1867],\n",
      "         [ 0.1815],\n",
      "         [ 0.1760],\n",
      "         [ 0.1702],\n",
      "         [ 0.1644],\n",
      "         [ 0.1586],\n",
      "         [ 0.1529],\n",
      "         [ 0.1473],\n",
      "         [ 0.1420],\n",
      "         [ 0.1371],\n",
      "         [ 0.1327],\n",
      "         [ 0.1287],\n",
      "         [ 0.1254],\n",
      "         [ 0.1228],\n",
      "         [ 0.1209],\n",
      "         [ 0.1199],\n",
      "         [ 0.1196],\n",
      "         [ 0.1203],\n",
      "         [ 0.1219],\n",
      "         [ 0.1244],\n",
      "         [ 0.1279],\n",
      "         [ 0.1323],\n",
      "         [ 0.1376],\n",
      "         [ 0.1437],\n",
      "         [ 0.1508],\n",
      "         [ 0.1586],\n",
      "         [ 0.1672],\n",
      "         [ 0.1764],\n",
      "         [ 0.1863],\n",
      "         [ 0.1966],\n",
      "         [ 0.2074],\n",
      "         [ 0.2185],\n",
      "         [ 0.2298],\n",
      "         [ 0.2412],\n",
      "         [ 0.2527],\n",
      "         [ 0.2640],\n",
      "         [ 0.2752],\n",
      "         [ 0.2860],\n",
      "         [ 0.2963],\n",
      "         [ 0.3062],\n",
      "         [ 0.3154],\n",
      "         [ 0.3239],\n",
      "         [ 0.3316],\n",
      "         [ 0.3383],\n",
      "         [ 0.3442],\n",
      "         [ 0.3490],\n",
      "         [ 0.3527],\n",
      "         [ 0.3553],\n",
      "         [ 0.3568],\n",
      "         [ 0.3572],\n",
      "         [ 0.3564],\n",
      "         [ 0.3544],\n",
      "         [ 0.3513],\n",
      "         [ 0.3470],\n",
      "         [ 0.3418],\n",
      "         [ 0.3354],\n",
      "         [ 0.3282],\n",
      "         [ 0.3200],\n",
      "         [ 0.3110],\n",
      "         [ 0.3012],\n",
      "         [ 0.2907],\n",
      "         [ 0.2797],\n",
      "         [ 0.2681],\n",
      "         [ 0.2561],\n",
      "         [ 0.2438],\n",
      "         [ 0.2312],\n",
      "         [ 0.2184],\n",
      "         [ 0.2056],\n",
      "         [ 0.1927],\n",
      "         [ 0.1799],\n",
      "         [ 0.1673],\n",
      "         [ 0.1548],\n",
      "         [ 0.1426],\n",
      "         [ 0.1306],\n",
      "         [ 0.1190],\n",
      "         [ 0.1078],\n",
      "         [ 0.0970],\n",
      "         [ 0.0866],\n",
      "         [ 0.0767],\n",
      "         [ 0.0673],\n",
      "         [ 0.0583],\n",
      "         [ 0.0498],\n",
      "         [ 0.0418],\n",
      "         [ 0.0342],\n",
      "         [ 0.0271],\n",
      "         [ 0.0205],\n",
      "         [ 0.0142],\n",
      "         [ 0.0084],\n",
      "         [ 0.0029],\n",
      "         [-0.0022],\n",
      "         [-0.0070],\n",
      "         [-0.0115],\n",
      "         [-0.0157],\n",
      "         [-0.0197],\n",
      "         [-0.0236],\n",
      "         [-0.0272],\n",
      "         [-0.0307],\n",
      "         [-0.0341],\n",
      "         [-0.0374],\n",
      "         [-0.0406],\n",
      "         [-0.0438],\n",
      "         [-0.0470],\n",
      "         [-0.0503],\n",
      "         [-0.0535],\n",
      "         [-0.0569],\n",
      "         [-0.0603],\n",
      "         [-0.0638],\n",
      "         [-0.0675],\n",
      "         [-0.0712],\n",
      "         [-0.0752],\n",
      "         [-0.0792],\n",
      "         [-0.0834],\n",
      "         [-0.0878],\n",
      "         [-0.0924],\n",
      "         [-0.0971],\n",
      "         [-0.1020],\n",
      "         [-0.1070],\n",
      "         [-0.1122],\n",
      "         [-0.1175],\n",
      "         [-0.1230],\n",
      "         [-0.1286],\n",
      "         [-0.1343],\n",
      "         [-0.1401],\n",
      "         [-0.1460],\n",
      "         [-0.1519],\n",
      "         [-0.1579],\n",
      "         [-0.1639],\n",
      "         [-0.1698],\n",
      "         [-0.1757],\n",
      "         [-0.1815],\n",
      "         [-0.1873],\n",
      "         [-0.1928],\n",
      "         [-0.1983],\n",
      "         [-0.2035],\n",
      "         [-0.2085],\n",
      "         [-0.2132],\n",
      "         [-0.2176],\n",
      "         [-0.2217],\n",
      "         [-0.2255],\n",
      "         [-0.2289],\n",
      "         [-0.2319],\n",
      "         [-0.2345],\n",
      "         [-0.2367],\n",
      "         [-0.2384],\n",
      "         [-0.2396],\n",
      "         [-0.2404],\n",
      "         [-0.2408],\n",
      "         [-0.2406],\n",
      "         [-0.2401],\n",
      "         [-0.2390],\n",
      "         [-0.2376],\n",
      "         [-0.2358],\n",
      "         [-0.2336],\n",
      "         [-0.2311],\n",
      "         [-0.2283],\n",
      "         [-0.2253],\n",
      "         [-0.2220],\n",
      "         [-0.2187],\n",
      "         [-0.2152],\n",
      "         [-0.2118],\n",
      "         [-0.2084],\n",
      "         [-0.2051],\n",
      "         [-0.2020],\n",
      "         [-0.1992],\n",
      "         [-0.1967],\n",
      "         [-0.1946],\n",
      "         [-0.1929],\n",
      "         [-0.1918],\n",
      "         [-0.1913],\n",
      "         [-0.1914],\n",
      "         [-0.1923],\n",
      "         [-0.1939],\n",
      "         [-0.1962],\n",
      "         [-0.1994],\n",
      "         [-0.2035],\n",
      "         [-0.2084],\n",
      "         [-0.2141],\n",
      "         [-0.2208],\n",
      "         [-0.2282],\n",
      "         [-0.2365],\n",
      "         [-0.2456],\n",
      "         [-0.2554],\n",
      "         [-0.2659],\n",
      "         [-0.2770],\n",
      "         [-0.2887],\n",
      "         [-0.3008],\n",
      "         [-0.3132],\n",
      "         [-0.3259],\n",
      "         [-0.3387],\n",
      "         [-0.3515],\n",
      "         [-0.3642],\n",
      "         [-0.3767],\n",
      "         [-0.3888],\n",
      "         [-0.4004],\n",
      "         [-0.4113],\n",
      "         [-0.4214],\n",
      "         [-0.4306],\n",
      "         [-0.4387],\n",
      "         [-0.4457],\n",
      "         [-0.4514],\n",
      "         [-0.4556],\n",
      "         [-0.4584],\n",
      "         [-0.4595],\n",
      "         [-0.4590],\n",
      "         [-0.4567],\n",
      "         [-0.4526],\n",
      "         [-0.4468],\n",
      "         [-0.4391],\n",
      "         [-0.4297],\n",
      "         [-0.4185],\n",
      "         [-0.4056],\n",
      "         [-0.3910],\n",
      "         [-0.3750],\n",
      "         [-0.3575],\n",
      "         [-0.3389],\n",
      "         [-0.3191],\n",
      "         [-0.2984],\n",
      "         [-0.2771],\n",
      "         [-0.2553],\n",
      "         [-0.2332],\n",
      "         [-0.2112],\n",
      "         [-0.1894],\n",
      "         [-0.1682],\n",
      "         [-0.1478],\n",
      "         [-0.1284],\n",
      "         [-0.1103],\n",
      "         [-0.0937],\n",
      "         [-0.0789],\n",
      "         [-0.0661],\n",
      "         [-0.0554],\n",
      "         [-0.0471],\n",
      "         [-0.0411],\n",
      "         [-0.0377],\n",
      "         [-0.0369],\n",
      "         [-0.0386],\n",
      "         [-0.0430],\n",
      "         [-0.0499],\n",
      "         [-0.0592],\n",
      "         [-0.0709],\n",
      "         [-0.0848],\n",
      "         [-0.1007],\n",
      "         [-0.1184],\n",
      "         [-0.1377],\n",
      "         [-0.1584],\n",
      "         [-0.1801],\n",
      "         [-0.2028],\n",
      "         [-0.2260],\n",
      "         [-0.2496],\n",
      "         [-0.2732],\n",
      "         [-0.2968],\n",
      "         [-0.3199],\n",
      "         [-0.3425],\n",
      "         [-0.3642],\n",
      "         [-0.3850],\n",
      "         [-0.4046],\n",
      "         [-0.4230],\n",
      "         [-0.4400],\n",
      "         [-0.4555],\n",
      "         [-0.4694],\n",
      "         [-0.4817],\n",
      "         [-0.4923],\n",
      "         [-0.5013],\n",
      "         [-0.5086],\n",
      "         [-0.5142],\n",
      "         [-0.5183],\n",
      "         [-0.5208],\n",
      "         [-0.5218],\n",
      "         [-0.5214],\n",
      "         [-0.5198],\n",
      "         [-0.5169],\n",
      "         [-0.5129],\n",
      "         [-0.5079],\n",
      "         [-0.5020],\n",
      "         [-0.4954],\n",
      "         [-0.4881],\n",
      "         [-0.4804],\n",
      "         [-0.4722],\n",
      "         [-0.4637],\n",
      "         [-0.4550],\n",
      "         [-0.4462],\n",
      "         [-0.4375],\n",
      "         [-0.4289],\n",
      "         [-0.4204],\n",
      "         [-0.4122],\n",
      "         [-0.4044],\n",
      "         [-0.3970],\n",
      "         [-0.3900],\n",
      "         [-0.3835],\n",
      "         [-0.3775],\n",
      "         [-0.3721],\n",
      "         [-0.3673],\n",
      "         [-0.3630],\n",
      "         [-0.3593],\n",
      "         [-0.3561],\n",
      "         [-0.3535],\n",
      "         [-0.3514],\n",
      "         [-0.3498],\n",
      "         [-0.3486],\n",
      "         [-0.3479],\n",
      "         [-0.3475],\n",
      "         [-0.3474],\n",
      "         [-0.3476],\n",
      "         [-0.3479],\n",
      "         [-0.3485],\n",
      "         [-0.3491],\n",
      "         [-0.3497],\n",
      "         [-0.3503],\n",
      "         [-0.3508],\n",
      "         [-0.3512],\n",
      "         [-0.3514],\n",
      "         [-0.3514],\n",
      "         [-0.3510],\n",
      "         [-0.3504],\n",
      "         [-0.3494],\n",
      "         [-0.3480],\n",
      "         [-0.3463],\n",
      "         [-0.3441],\n",
      "         [-0.3414],\n",
      "         [-0.3384],\n",
      "         [-0.3349],\n",
      "         [-0.3310],\n",
      "         [-0.3268],\n",
      "         [-0.3221],\n",
      "         [-0.3171],\n",
      "         [-0.3118],\n",
      "         [-0.3063],\n",
      "         [-0.3005],\n",
      "         [-0.2946],\n",
      "         [-0.2885],\n",
      "         [-0.2825],\n",
      "         [-0.2764],\n",
      "         [-0.2704],\n",
      "         [-0.2646],\n",
      "         [-0.2590],\n",
      "         [-0.2537],\n",
      "         [-0.2487],\n",
      "         [-0.2441],\n",
      "         [-0.2400],\n",
      "         [-0.2364],\n",
      "         [-0.2334],\n",
      "         [-0.2310],\n",
      "         [-0.2292],\n",
      "         [-0.2282],\n",
      "         [-0.2279],\n",
      "         [-0.2283],\n",
      "         [-0.2295],\n",
      "         [-0.2315],\n",
      "         [-0.2343],\n",
      "         [-0.2378],\n",
      "         [-0.2421],\n",
      "         [-0.2472],\n",
      "         [-0.2529],\n",
      "         [-0.2593],\n",
      "         [-0.2664],\n",
      "         [-0.2740],\n",
      "         [-0.2822],\n",
      "         [-0.2909],\n",
      "         [-0.2999],\n",
      "         [-0.3094],\n",
      "         [-0.3191],\n",
      "         [-0.3291],\n",
      "         [-0.3392],\n",
      "         [-0.3494],\n",
      "         [-0.3596],\n",
      "         [-0.3698],\n",
      "         [-0.3799],\n",
      "         [-0.3898],\n",
      "         [-0.3995],\n",
      "         [-0.4089],\n",
      "         [-0.4180],\n",
      "         [-0.4267],\n",
      "         [-0.4350],\n",
      "         [-0.4428],\n",
      "         [-0.4502],\n",
      "         [-0.4570],\n",
      "         [-0.4634],\n",
      "         [-0.4692],\n",
      "         [-0.4744],\n",
      "         [-0.4791],\n",
      "         [-0.4832],\n",
      "         [-0.4867],\n",
      "         [-0.4897],\n",
      "         [-0.4922],\n",
      "         [-0.4942],\n",
      "         [-0.4957],\n",
      "         [-0.4966],\n",
      "         [-0.4972],\n",
      "         [-0.4973],\n",
      "         [-0.4969],\n",
      "         [-0.4963],\n",
      "         [-0.4952],\n",
      "         [-0.4939],\n",
      "         [-0.4922],\n",
      "         [-0.4903],\n",
      "         [-0.4881],\n",
      "         [-0.4858],\n",
      "         [-0.4832],\n",
      "         [-0.4805],\n",
      "         [-0.4777],\n",
      "         [-0.4747],\n",
      "         [-0.4716],\n",
      "         [-0.4684],\n",
      "         [-0.4652],\n",
      "         [-0.4619],\n",
      "         [-0.4586],\n",
      "         [-0.4552],\n",
      "         [-0.4518],\n",
      "         [-0.4484],\n",
      "         [-0.4449],\n",
      "         [-0.4415],\n",
      "         [-0.4380],\n",
      "         [-0.4345],\n",
      "         [-0.4310],\n",
      "         [-0.4275],\n",
      "         [-0.4239],\n",
      "         [-0.4204],\n",
      "         [-0.4168],\n",
      "         [-0.4132],\n",
      "         [-0.4096],\n",
      "         [-0.4059],\n",
      "         [-0.4022],\n",
      "         [-0.3984],\n",
      "         [-0.3946],\n",
      "         [-0.3908],\n",
      "         [-0.3869],\n",
      "         [-0.3830],\n",
      "         [-0.3791],\n",
      "         [-0.3750],\n",
      "         [-0.3710],\n",
      "         [-0.3669],\n",
      "         [-0.3628],\n",
      "         [-0.3586],\n",
      "         [-0.3544],\n",
      "         [-0.3502],\n",
      "         [-0.3460],\n",
      "         [-0.3418],\n",
      "         [-0.3375],\n",
      "         [-0.3333],\n",
      "         [-0.3290],\n",
      "         [-0.3248],\n",
      "         [-0.3206],\n",
      "         [-0.3164],\n",
      "         [-0.3123],\n",
      "         [-0.3083],\n",
      "         [-0.3043],\n",
      "         [-0.3004],\n",
      "         [-0.2966],\n",
      "         [-0.2928],\n",
      "         [-0.2892],\n",
      "         [-0.2857],\n",
      "         [-0.2823],\n",
      "         [-0.2790],\n",
      "         [-0.2759],\n",
      "         [-0.2729],\n",
      "         [-0.2700],\n",
      "         [-0.2674],\n",
      "         [-0.2648],\n",
      "         [-0.2624],\n",
      "         [-0.2602],\n",
      "         [-0.2581],\n",
      "         [-0.2562],\n",
      "         [-0.2544],\n",
      "         [-0.2527],\n",
      "         [-0.2513],\n",
      "         [-0.2499],\n",
      "         [-0.2487],\n",
      "         [-0.2476],\n",
      "         [-0.2467],\n",
      "         [-0.2458],\n",
      "         [-0.2451],\n",
      "         [-0.2445],\n",
      "         [-0.2439],\n",
      "         [-0.2435],\n",
      "         [-0.2431],\n",
      "         [-0.2428],\n",
      "         [-0.2426],\n",
      "         [-0.2425],\n",
      "         [-0.2424],\n",
      "         [-0.2423],\n",
      "         [-0.2423],\n",
      "         [-0.2424],\n",
      "         [-0.2425],\n",
      "         [-0.2427],\n",
      "         [-0.2429],\n",
      "         [-0.2431],\n",
      "         [-0.2435],\n",
      "         [-0.2438],\n",
      "         [-0.2443],\n",
      "         [-0.2447],\n",
      "         [-0.2453],\n",
      "         [-0.2459],\n",
      "         [-0.2466],\n",
      "         [-0.2474],\n",
      "         [-0.2482],\n",
      "         [-0.2491],\n",
      "         [-0.2501],\n",
      "         [-0.2512],\n",
      "         [-0.2523],\n",
      "         [-0.2535],\n",
      "         [-0.2547],\n",
      "         [-0.2560],\n",
      "         [-0.2573],\n",
      "         [-0.2586],\n",
      "         [-0.2600],\n",
      "         [-0.2613],\n",
      "         [-0.2626],\n",
      "         [-0.2639],\n",
      "         [-0.2651],\n",
      "         [-0.2662],\n",
      "         [-0.2671],\n",
      "         [-0.2680],\n",
      "         [-0.2686],\n",
      "         [-0.2691],\n",
      "         [-0.2693],\n",
      "         [-0.2693],\n",
      "         [-0.2690],\n",
      "         [-0.2685],\n",
      "         [-0.2676],\n",
      "         [-0.2664],\n",
      "         [-0.2648],\n",
      "         [-0.2628],\n",
      "         [-0.2604],\n",
      "         [-0.2577],\n",
      "         [-0.2545],\n",
      "         [-0.2508],\n",
      "         [-0.2468],\n",
      "         [-0.2423],\n",
      "         [-0.2374],\n",
      "         [-0.2320],\n",
      "         [-0.2263],\n",
      "         [-0.2201],\n",
      "         [-0.2136],\n",
      "         [-0.2067],\n",
      "         [-0.1995],\n",
      "         [-0.1921],\n",
      "         [-0.1843],\n",
      "         [-0.1764],\n",
      "         [-0.1683],\n",
      "         [-0.1601],\n",
      "         [-0.1519],\n",
      "         [-0.1436],\n",
      "         [-0.1354],\n",
      "         [-0.1274],\n",
      "         [-0.1195],\n",
      "         [-0.1119],\n",
      "         [-0.1046],\n",
      "         [-0.0977],\n",
      "         [-0.0913],\n",
      "         [-0.0854],\n",
      "         [-0.0800],\n",
      "         [-0.0753],\n",
      "         [-0.0712],\n",
      "         [-0.0679],\n",
      "         [-0.0653],\n",
      "         [-0.0635],\n",
      "         [-0.0625],\n",
      "         [-0.0624],\n",
      "         [-0.0631],\n",
      "         [-0.0646],\n",
      "         [-0.0670],\n",
      "         [-0.0702],\n",
      "         [-0.0741],\n",
      "         [-0.0788],\n",
      "         [-0.0841],\n",
      "         [-0.0901],\n",
      "         [-0.0966],\n",
      "         [-0.1035],\n",
      "         [-0.1109],\n",
      "         [-0.1184],\n",
      "         [-0.1262],\n",
      "         [-0.1339],\n",
      "         [-0.1416],\n",
      "         [-0.1491],\n",
      "         [-0.1562],\n",
      "         [-0.1629],\n",
      "         [-0.1690],\n",
      "         [-0.1743],\n",
      "         [-0.1788],\n",
      "         [-0.1823],\n",
      "         [-0.1847],\n",
      "         [-0.1859],\n",
      "         [-0.1858],\n",
      "         [-0.1843],\n",
      "         [-0.1813],\n",
      "         [-0.1767],\n",
      "         [-0.1706],\n",
      "         [-0.1628],\n",
      "         [-0.1535],\n",
      "         [-0.1425],\n",
      "         [-0.1299],\n",
      "         [-0.1158],\n",
      "         [-0.1002],\n",
      "         [-0.0832],\n",
      "         [-0.0651],\n",
      "         [-0.0458],\n",
      "         [-0.0256],\n",
      "         [-0.0047],\n",
      "         [ 0.0168],\n",
      "         [ 0.0387],\n",
      "         [ 0.0606],\n",
      "         [ 0.0824],\n",
      "         [ 0.1038],\n",
      "         [ 0.1246],\n",
      "         [ 0.1445],\n",
      "         [ 0.1632],\n",
      "         [ 0.1806],\n",
      "         [ 0.1964],\n",
      "         [ 0.2104],\n",
      "         [ 0.2223],\n",
      "         [ 0.2321],\n",
      "         [ 0.2396],\n",
      "         [ 0.2447],\n",
      "         [ 0.2473],\n",
      "         [ 0.2474],\n",
      "         [ 0.2449],\n",
      "         [ 0.2399],\n",
      "         [ 0.2325],\n",
      "         [ 0.2227],\n",
      "         [ 0.2107],\n",
      "         [ 0.1967],\n",
      "         [ 0.1807],\n",
      "         [ 0.1631],\n",
      "         [ 0.1440],\n",
      "         [ 0.1237],\n",
      "         [ 0.1025],\n",
      "         [ 0.0804],\n",
      "         [ 0.0579],\n",
      "         [ 0.0352],\n",
      "         [ 0.0124],\n",
      "         [-0.0102],\n",
      "         [-0.0323],\n",
      "         [-0.0538],\n",
      "         [-0.0746],\n",
      "         [-0.0944],\n",
      "         [-0.1131],\n",
      "         [-0.1306],\n",
      "         [-0.1468],\n",
      "         [-0.1617],\n",
      "         [-0.1751],\n",
      "         [-0.1871],\n",
      "         [-0.1976],\n",
      "         [-0.2067],\n",
      "         [-0.2144],\n",
      "         [-0.2206],\n",
      "         [-0.2256],\n",
      "         [-0.2292],\n",
      "         [-0.2317],\n",
      "         [-0.2331],\n",
      "         [-0.2335],\n",
      "         [-0.2329],\n",
      "         [-0.2316],\n",
      "         [-0.2295],\n",
      "         [-0.2269],\n",
      "         [-0.2237],\n",
      "         [-0.2201],\n",
      "         [-0.2162],\n",
      "         [-0.2121],\n",
      "         [-0.2079],\n",
      "         [-0.2036],\n",
      "         [-0.1993],\n",
      "         [-0.1950],\n",
      "         [-0.1909],\n",
      "         [-0.1870],\n",
      "         [-0.1832],\n",
      "         [-0.1797],\n",
      "         [-0.1765],\n",
      "         [-0.1736],\n",
      "         [-0.1709],\n",
      "         [-0.1686],\n",
      "         [-0.1665],\n",
      "         [-0.1647],\n",
      "         [-0.1632],\n",
      "         [-0.1619],\n",
      "         [-0.1608],\n",
      "         [-0.1600],\n",
      "         [-0.1593],\n",
      "         [-0.1588],\n",
      "         [-0.1583],\n",
      "         [-0.1580],\n",
      "         [-0.1577],\n",
      "         [-0.1574],\n",
      "         [-0.1570],\n",
      "         [-0.1567],\n",
      "         [-0.1562],\n",
      "         [-0.1557],\n",
      "         [-0.1550],\n",
      "         [-0.1541],\n",
      "         [-0.1531],\n",
      "         [-0.1519],\n",
      "         [-0.1505],\n",
      "         [-0.1489],\n",
      "         [-0.1470],\n",
      "         [-0.1449],\n",
      "         [-0.1425],\n",
      "         [-0.1399],\n",
      "         [-0.1371],\n",
      "         [-0.1340],\n",
      "         [-0.1307],\n",
      "         [-0.1272],\n",
      "         [-0.1234],\n",
      "         [-0.1195],\n",
      "         [-0.1153],\n",
      "         [-0.1109],\n",
      "         [-0.1064],\n",
      "         [-0.1017],\n",
      "         [-0.0968],\n",
      "         [-0.0918],\n",
      "         [-0.0867],\n",
      "         [-0.0815],\n",
      "         [-0.0762],\n",
      "         [-0.0708],\n",
      "         [-0.0654],\n",
      "         [-0.0599],\n",
      "         [-0.0544],\n",
      "         [-0.0489],\n",
      "         [-0.0434],\n",
      "         [-0.0379],\n",
      "         [-0.0324],\n",
      "         [-0.0270],\n",
      "         [-0.0217],\n",
      "         [-0.0165],\n",
      "         [-0.0114],\n",
      "         [-0.0064],\n",
      "         [-0.0015],\n",
      "         [ 0.0032],\n",
      "         [ 0.0077],\n",
      "         [ 0.0120],\n",
      "         [ 0.0161],\n",
      "         [ 0.0199],\n",
      "         [ 0.0235],\n",
      "         [ 0.0268],\n",
      "         [ 0.0298],\n",
      "         [ 0.0325]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (batch_X_test, batch_y_test) in enumerate(test_loader):\n",
    "    print(batch_y_test.shape)\n",
    "print(batch_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrifurrer/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(step_output)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Append the prediction to tgt_input_val for the next timestep\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     tgt_input_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([tgt_input_val, step_output], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Combine all timestep predictions\u001b[39;00m\n\u001b[1;32m     46\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over the validation set in batches\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "with torch.no_grad():\n",
    "    if use_dataloader:\n",
    "        # Iterate through the batches in the test_loader to load the data in batches\n",
    "        for batch_idx, (batch_X_test, batch_y_test) in enumerate(test_loader):\n",
    "            # Move the batch data to the device (GPU or CPU)\n",
    "            batch_X_test = batch_X_test.to(device)\n",
    "            batch_y_test = batch_y_test.to(device)\n",
    "            \n",
    "            # Get the start and end index of the current batch in df_test\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + len(batch_X_test)\n",
    "            \n",
    "            # Retrieve the corresponding (subject, action) pair for this batch from df_test\n",
    "            batch_subjects = df_test.iloc[start_idx:end_idx]['subject'].values\n",
    "            batch_actions = df_test.iloc[start_idx:end_idx]['action'].values\n",
    "\n",
    "            # Initialize decoder input with the start token (all zeros)\n",
    "            tgt_input_val = torch.zeros((batch_y_test.size(0), 1, batch_y_test.size(-1)), device=device)\n",
    "\n",
    "            # Autoregressive decoding\n",
    "            predictions = []\n",
    "            for _ in range(batch_y_test.size(1)):\n",
    "                # Generate tgt_mask for the current step\n",
    "                tgt_mask = generate_square_subsequent_mask(tgt_input_val.size(1)).to(device)\n",
    "\n",
    "                # Generate key padding masks\n",
    "                src_key_padding_mask = (batch_X_test[:, :, 0] == 0).to(device)\n",
    "                tgt_key_padding_mask = (tgt_input_val.squeeze(-1) == 0).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                step_output = model(\n",
    "                    batch_X_test,\n",
    "                    tgt_input_val,\n",
    "                    tgt_mask=tgt_mask,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                )[:, -1:, :]  # Take the output for the last timestep\n",
    "                predictions.append(step_output)\n",
    "\n",
    "                # Append the prediction to tgt_input_val for the next timestep\n",
    "                tgt_input_val = torch.cat([tgt_input_val, step_output], dim=1)\n",
    "\n",
    "            # Combine all timestep predictions\n",
    "            batch_predictions = torch.cat(predictions, dim=1)\n",
    "\n",
    "            # Create the target input (tgt) for the decoder using teacher forcing\n",
    "            # tgt_input_val = torch.cat(\n",
    "            #     [torch.zeros((batch_y_test.size(0), 1, batch_y_test.size(-1)), device=device),  # Start token\n",
    "            #     batch_y_test[:, :-1, :]],  # Shifted target sequence\n",
    "            #     dim=1\n",
    "            # )\n",
    "\n",
    "            # # Generate tgt_mask \n",
    "            # # Shape: [seq_len_tgt, seq_len_tgt]\n",
    "            # tgt_mask = generate_square_subsequent_mask(tgt_input_val.size(1)).to(device)\n",
    "\n",
    "            # # Generate src_key_padding_mask \n",
    "            # # Shape: [batch_size, seq_len_src]\n",
    "            # src_key_padding_mask = (batch_X_test[:, :, 0] == 0).to(device) # Use only one feature (i.e. one ppg signal)\n",
    "\n",
    "            # # Generate tgt_key_padding_mask (pad tokens are 0 in the target)\n",
    "            # # Shape: [batch_size, seq_len_tgt]\n",
    "            # tgt_key_padding_mask = (tgt_input_val.squeeze(-1) == 0).to(device)\n",
    "\n",
    "            # # Debugging: print mask shapes to ensure they are correct\n",
    "            # # assert src_key_padding_mask.shape == [batch_size, sequence_length]\n",
    "            # # assert src_key_padding_mask.shape == [batch_size, sequence_length]\n",
    "            # # print(\"seq_len_src\", sequence_length)\n",
    "            # # print(\"tgt_len\", sequence_length)\n",
    "            # # print(f\"src_key_padding_mask shape: {src_key_padding_mask.shape}\")  # Should be [batch_size, seq_len_src]\n",
    "            # # print(f\"tgt_key_padding_mask shape: {tgt_key_padding_mask.shape}\")  # Should be [batch_size, seq_len_tgt]\n",
    "\n",
    "            # memory_mask = None\n",
    "\n",
    "            # # Forward pass to get predictions\n",
    "            # batch_predictions = model(\n",
    "            #     batch_X_test, \n",
    "            #     tgt_input_val, \n",
    "            #     tgt_mask=tgt_mask, \n",
    "            #     src_key_padding_mask=src_key_padding_mask, \n",
    "            #     tgt_key_padding_mask=tgt_key_padding_mask\n",
    "            # )\n",
    "            # Calculate loss for this batch\n",
    "            loss = loss_fn(batch_predictions, batch_y_test)\n",
    "\n",
    "            # Accumulate total validation loss\n",
    "            running_test_loss += loss.item() * batch_X_test.size(0)\n",
    "\n",
    "            # Store predictions, actuals, subjects, and actions\n",
    "            ecg_predictions.append(batch_predictions.cpu())  # Move to CPU for numpy/scaler operations\n",
    "            ecg_actuals.append(batch_y_test.cpu())\n",
    "            ppg.append(batch_X_test.cpu())\n",
    "            subjects.extend(batch_subjects)\n",
    "            actions.extend(batch_actions)\n",
    "\n",
    "    else:\n",
    "        for j in range(0, len(X_test), batch_size):\n",
    "            # Get the current validation batch\n",
    "            batch_X_test = X_test[j:j + batch_size].to(device)\n",
    "            batch_y_test = y_test[j:j + batch_size].to(device)\n",
    "\n",
    "            # Retrieve subject and action for the batch\n",
    "            batch_subjects = df_test.iloc[j:j + batch_size]['subject'].values\n",
    "            batch_actions = df_test.iloc[j:j + batch_size]['action'].values\n",
    "\n",
    "            # Create the target input (tgt) for the decoder using teacher forcing\n",
    "            tgt_input_val = torch.cat(\n",
    "                [torch.zeros((batch_y_test.size(0), 1, batch_y_test.size(-1)), device=device),  # Start token\n",
    "                batch_y_test[:, :-1, :]],  # Shifted target sequence\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # Generate tgt_mask \n",
    "            # Shape: [seq_len_tgt, seq_len_tgt]\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_input_val.size(1)).to(device)\n",
    "\n",
    "            # Generate src_key_padding_mask \n",
    "            # Shape: [batch_size, seq_len_src]\n",
    "            src_key_padding_mask = (batch_X_test[:, :, 0] == 0).to(device) # Use only one feature (i.e. one ppg signal)\n",
    "\n",
    "            # Generate tgt_key_padding_mask (pad tokens are 0 in the target)\n",
    "            # Shape: [batch_size, seq_len_tgt]\n",
    "            tgt_key_padding_mask = (tgt_input_val.squeeze(-1) == 0).to(device)\n",
    "\n",
    "            # Debugging: print mask shapes to ensure they are correct\n",
    "            # assert src_key_padding_mask.shape == [batch_size, sequence_length]\n",
    "            # assert src_key_padding_mask.shape == [batch_size, sequence_length]\n",
    "            # print(\"seq_len_src\", sequence_length)\n",
    "            # print(\"tgt_len\", sequence_length)\n",
    "            # print(f\"src_key_padding_mask shape: {src_key_padding_mask.shape}\")  # Should be [batch_size, seq_len_src]\n",
    "            # print(f\"tgt_key_padding_mask shape: {tgt_key_padding_mask.shape}\")  # Should be [batch_size, seq_len_tgt]\n",
    "\n",
    "            memory_mask = None\n",
    "\n",
    "            # Forward pass to get predictions\n",
    "            batch_predictions = model(\n",
    "                batch_X_test, \n",
    "                tgt_input_val, \n",
    "                tgt_mask=tgt_mask, \n",
    "                src_key_padding_mask=src_key_padding_mask, \n",
    "                tgt_key_padding_mask=tgt_key_padding_mask\n",
    "            )\n",
    "            # Calculate loss for this batch\n",
    "            loss = loss_fn(batch_predictions, batch_y_test)\n",
    "\n",
    "            # Accumulate total validation loss\n",
    "            running_test_loss += loss.item() * batch_X_test.size(0)\n",
    "\n",
    "            # Store predictions, actuals, subjects, and actions\n",
    "            ecg_predictions.append(batch_predictions.cpu())  # Move to CPU for numpy/scaler operations\n",
    "            ecg_actuals.append(batch_y_test.cpu())\n",
    "            ppg.append(batch_X_test.cpu())\n",
    "            subjects.extend(batch_subjects)\n",
    "            actions.extend(batch_actions)\n",
    "\n",
    "# Average the test loss over all samples\n",
    "avg_test_loss = running_test_loss / len(X_test)\n",
    "test_rmse = torch.sqrt(torch.tensor(avg_test_loss))\n",
    "test_loss = np.append(test_loss, test_rmse.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "ecg_predictions = torch.cat(ecg_predictions, dim=0)\n",
    "ecg_actuals = torch.cat(ecg_actuals, dim=0)\n",
    "ppg = torch.cat(ppg, dim=0)\n",
    "\n",
    "# Initialize lists for original scale data\n",
    "ecg_predictions_original_scale = []\n",
    "ecg_actuals_original_scale = []\n",
    "ppg_original_scale = []\n",
    "\n",
    "# Process each sequence\n",
    "for i in range(len(ecg_predictions)):\n",
    "    # Get subject and action for the current sequence\n",
    "    subject = subjects[i]\n",
    "    action = actions[i]\n",
    "\n",
    "    # Retrieve the correct scalers\n",
    "    scaler_input = scalers[(subject, action)]['input_scaler']\n",
    "    scaler_target = scalers[(subject, action)]['target_scaler']\n",
    "\n",
    "    # Inverse transform predictions and actuals for the current sequence\n",
    "    ecg_pred = ecg_predictions[i].squeeze(-1).numpy()  # Shape: [sequence_length]\n",
    "    ecg_act = ecg_actuals[i].squeeze(-1).numpy()       # Shape: [sequence_length]\n",
    "    ppg_seq = ppg[i].numpy()                          # Shape: [sequence_length, 3]\n",
    "\n",
    "    ecg_predictions_original_scale.append(scaler_target.inverse_transform(ecg_pred.reshape(-1, 1)).flatten())\n",
    "    ecg_actuals_original_scale.append(scaler_target.inverse_transform(ecg_act.reshape(-1, 1)).flatten())\n",
    "    ppg_original_scale.append(scaler_input.inverse_transform(ppg_seq))\n",
    "\n",
    "# Convert back to arrays\n",
    "ecg_predictions_original_scale = np.array(ecg_predictions_original_scale)\n",
    "ecg_actuals_original_scale = np.array(ecg_actuals_original_scale)\n",
    "ppg_original_scale = np.array(ppg_original_scale)\n",
    "\n",
    "# Separate PPG channels \n",
    "red_ppg = ppg_original_scale[:, :, 0]  # Red PPG\n",
    "ir_ppg = ppg_original_scale[:, :, 1]   # IR PPG\n",
    "green_ppg = ppg_original_scale[:, :, 2]  # Green PPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation\n",
    "# Initialize storage for aggregated predictions and actual values\n",
    "ecg_predictions = []\n",
    "ecg_actuals = []\n",
    "ppg = []\n",
    "subjects = []  # Store subject info for each batch\n",
    "actions = []   # Store action info for each batch\n",
    "\n",
    "# Loss function: Mean Squared Error for regression tasks\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Test Loss\n",
    "test_loss = np.array([])\n",
    "running_test_loss = 0\n",
    "\n",
    "# Iterate over the validation set in batches\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "with torch.no_grad():\n",
    "    for j in range(0, len(X_test), batch_size):\n",
    "        # Get the current validation batch\n",
    "        batch_X_test = X_test[j:j + batch_size].to(device)\n",
    "        batch_y_test = y_test[j:j + batch_size].to(device)\n",
    "\n",
    "        # Retrieve subject and action for the batch\n",
    "        batch_subjects = df_test.iloc[j:j + batch_size]['subject'].values\n",
    "        batch_actions = df_test.iloc[j:j + batch_size]['action'].values\n",
    "\n",
    "        # Create the target input (tgt) for the decoder using teacher forcing\n",
    "        tgt_input_val = torch.cat(\n",
    "            [torch.zeros((batch_y_test.size(0), 1, batch_y_test.size(-1)), device=device),  # Start token\n",
    "            batch_y_test[:, :-1, :]],  # Shifted target sequence\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        # Generate tgt_mask \n",
    "        # Shape: [seq_len_tgt, seq_len_tgt]\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_input_val.size(1)).to(device)\n",
    "\n",
    "        # Generate src_key_padding_mask \n",
    "        # Shape: [batch_size, seq_len_src]\n",
    "        src_key_padding_mask = (batch_X_test[:, :, 0] == 0).to(device) # Use only one feature (i.e. one ppg signal)\n",
    "\n",
    "        # Generate tgt_key_padding_mask (pad tokens are 0 in the target)\n",
    "        # Shape: [batch_size, seq_len_tgt]\n",
    "        tgt_key_padding_mask = (tgt_input_val.squeeze(-1) == 0).to(device)\n",
    "\n",
    "        # Debugging: print mask shapes to ensure they are correct\n",
    "        # assert src_key_padding_mask.shape == [batch_size, sequence_length]\n",
    "        # assert src_key_padding_mask.shape == [batch_size, sequence_length]\n",
    "        # print(\"seq_len_src\", sequence_length)\n",
    "        # print(\"tgt_len\", sequence_length)\n",
    "        # print(f\"src_key_padding_mask shape: {src_key_padding_mask.shape}\")  # Should be [batch_size, seq_len_src]\n",
    "        # print(f\"tgt_key_padding_mask shape: {tgt_key_padding_mask.shape}\")  # Should be [batch_size, seq_len_tgt]\n",
    "\n",
    "        memory_mask = None\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        batch_predictions = model(\n",
    "            batch_X_test, \n",
    "            tgt_input_val, \n",
    "            tgt_mask=tgt_mask, \n",
    "            src_key_padding_mask=src_key_padding_mask, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        # Calculate loss for this batch\n",
    "        loss = loss_fn(batch_predictions, batch_y_test)\n",
    "\n",
    "        # Accumulate total validation loss\n",
    "        running_test_loss += loss.item() * batch_X_test.size(0)\n",
    "\n",
    "        # Store predictions, actuals, subjects, and actions\n",
    "        ecg_predictions.append(batch_predictions.cpu())  # Move to CPU for numpy/scaler operations\n",
    "        ecg_actuals.append(batch_y_test.cpu())\n",
    "        ppg.append(batch_X_test.cpu())\n",
    "        subjects.extend(batch_subjects)\n",
    "        actions.extend(batch_actions)\n",
    "\n",
    "# Average the test loss over all samples\n",
    "avg_test_loss = running_test_loss / len(X_test)\n",
    "test_rmse = torch.sqrt(torch.tensor(avg_test_loss))\n",
    "test_loss = np.append(test_loss, test_rmse.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "ecg_predictions = torch.cat(ecg_predictions, dim=0)\n",
    "ecg_actuals = torch.cat(ecg_actuals, dim=0)\n",
    "ppg = torch.cat(ppg, dim=0)\n",
    "\n",
    "# Initialize lists for original scale data\n",
    "ecg_predictions_original_scale = []\n",
    "ecg_actuals_original_scale = []\n",
    "ppg_original_scale = []\n",
    "\n",
    "# Process each sequence\n",
    "for i in range(len(ecg_predictions)):\n",
    "    # Get subject and action for the current sequence\n",
    "    subject = subjects[i]\n",
    "    action = actions[i]\n",
    "\n",
    "    # Retrieve the correct scalers\n",
    "    scaler_input = scalers[(subject, action)]['input_scaler']\n",
    "    scaler_target = scalers[(subject, action)]['target_scaler']\n",
    "\n",
    "    # Inverse transform predictions and actuals for the current sequence\n",
    "    ecg_pred = ecg_predictions[i].squeeze(-1).numpy()  # Shape: [sequence_length]\n",
    "    ecg_act = ecg_actuals[i].squeeze(-1).numpy()       # Shape: [sequence_length]\n",
    "    ppg_seq = ppg[i].numpy()                          # Shape: [sequence_length, 3]\n",
    "\n",
    "    ecg_predictions_original_scale.append(scaler_target.inverse_transform(ecg_pred.reshape(-1, 1)).flatten())\n",
    "    ecg_actuals_original_scale.append(scaler_target.inverse_transform(ecg_act.reshape(-1, 1)).flatten())\n",
    "    ppg_original_scale.append(scaler_input.inverse_transform(ppg_seq))\n",
    "\n",
    "# Convert back to arrays\n",
    "ecg_predictions_original_scale = np.array(ecg_predictions_original_scale)\n",
    "ecg_actuals_original_scale = np.array(ecg_actuals_original_scale)\n",
    "ppg_original_scale = np.array(ppg_original_scale)\n",
    "\n",
    "# Separate PPG channels \n",
    "red_ppg = ppg_original_scale[:, :, 0]  # Red PPG\n",
    "ir_ppg = ppg_original_scale[:, :, 1]   # IR PPG\n",
    "green_ppg = ppg_original_scale[:, :, 2]  # Green PPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalized Evaluation metrics\n",
    "# Predictions and actual values (normalized and flattened)\n",
    "ecg_predictions_arr = np.array(ecg_predictions).flatten()\n",
    "ecg_actuals_arr = np.array(ecg_actuals).flatten()\n",
    "\n",
    "# Calculate the range of the actual data for normalization\n",
    "actual_range_normalized = np.ptp(ecg_actuals)  # Peak-to-peak (max - min)\n",
    "\n",
    "# Euclidean Distance\n",
    "euclidean_distance_normalized = euclidean(ecg_predictions_arr, ecg_actuals_arr)\n",
    "\n",
    "# Dynamic Time Warping (DTW)\n",
    "downsampling_factor_dtw = 10\n",
    "batch_size_dtw = 10\n",
    "dtw_distance_normalized = compute_batched_dtw(ecg_predictions_arr, ecg_actuals_arr, batch_size_dtw, downsampling_factor_dtw)\n",
    "# dtw_distance = alignment.distance\n",
    "\n",
    "# Pearson Correlation\n",
    "pearson_corr_normalized, _ = pearsonr(ecg_predictions_arr, ecg_actuals_arr)\n",
    "\n",
    "# Spearman Correlation\n",
    "spearman_corr_normalized, _ = spearmanr(ecg_predictions_arr, ecg_actuals_arr)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_normalized = np.mean((ecg_predictions_arr - ecg_actuals_arr) ** 2)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_normalized = np.mean(np.abs(ecg_predictions_arr - ecg_actuals_arr))\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse_normalized = np.sqrt(mse_normalized)\n",
    "\n",
    "# Normalized Root Mean Squared Error (NRMSE)\n",
    "nrmse_normalized = rmse_normalized / actual_range_normalized\n",
    "\n",
    "# Normalized Mean Absolute Error (NMAE)\n",
    "nmae_normalized = mae_normalized / actual_range_normalized\n",
    "\n",
    "# Print metrics\n",
    "metrics_normalized = {\n",
    "    \"Euclidean Distance\": euclidean_distance_normalized,\n",
    "    \"DTW Distance\": dtw_distance_normalized,\n",
    "    \"Pearson Correlation\": pearson_corr_normalized,\n",
    "    \"Spearman Correlation\": spearman_corr_normalized,\n",
    "    \"MSE\": mse_normalized,\n",
    "    \"MAE\": mae_normalized,\n",
    "    \"RMSE\": rmse_normalized,\n",
    "    \"NRMSE\": nrmse_normalized,\n",
    "    \"NMAE\": nmae_normalized,\n",
    "}\n",
    "\n",
    "for metric, value in metrics_normalized.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Write metrics to a file\n",
    "metrics_file = os.path.join(results_folder, f\"metrics_s{config['parameters']['subject']}_{config['parameters']['action']}.txt\")\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    for metric, value in metrics_normalized.items():\n",
    "        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "\n",
    "print(f\"Metrics written to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation metrics\n",
    "# Predictions and actual values (already scaled back to original scale)\n",
    "ecg_predictions_original_scale_flattened = ecg_predictions_original_scale.flatten()  # Flatten to 1D if necessary\n",
    "ecg_actuals_original_scale_flattened = ecg_actuals_original_scale.flatten()\n",
    "\n",
    "# Calculate the range of the actual data for normalization\n",
    "actual_range = np.ptp(ecg_actuals_original_scale)  # Peak-to-peak (max - min)\n",
    "\n",
    "# Euclidean Distance\n",
    "euclidean_distance = euclidean(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened)\n",
    "\n",
    "# Dynamic Time Warping (DTW)\n",
    "downsampling_factor_dtw = 10\n",
    "batch_size_dtw = 10 \n",
    "dtw_distance = compute_batched_dtw(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened, batch_size_dtw, downsampling_factor_dtw)\n",
    "# dtw_distance = alignment.distance\n",
    "\n",
    "# Pearson Correlation\n",
    "pearson_corr, _ = pearsonr(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened)\n",
    "\n",
    "# Spearman Correlation\n",
    "spearman_corr, _ = spearmanr(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = np.mean((ecg_predictions_original_scale_flattened - ecg_actuals_original_scale_flattened) ** 2)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = np.mean(np.abs(ecg_predictions_original_scale_flattened - ecg_actuals_original_scale_flattened))\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Normalized Root Mean Squared Error (NRMSE)\n",
    "nrmse = rmse / actual_range\n",
    "\n",
    "# Normalized Mean Absolute Error (NMAE)\n",
    "nmae = mae / actual_range\n",
    "\n",
    "# Print metrics\n",
    "metrics = {\n",
    "    \"Euclidean Distance\": euclidean_distance,\n",
    "    \"DTW Distance\": dtw_distance,\n",
    "    \"Pearson Correlation\": pearson_corr,\n",
    "    \"Spearman Correlation\": spearman_corr,\n",
    "    \"MSE\": mse,\n",
    "    \"MAE\": mae,\n",
    "    \"RMSE\": rmse,\n",
    "    \"NRMSE\": nrmse,\n",
    "    \"NMAE\": nmae,\n",
    "}\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Write metrics to a file\n",
    "metrics_file = os.path.join(results_folder, f\"metrics_s{config['parameters']['subject']}_{config['parameters']['action']}.txt\")\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    for metric, value in metrics.items():\n",
    "        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "\n",
    "print(f\"Metrics written to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating graphs\n",
    "# Randomly select an index from the validation data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss,  label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.title(f\"Training and Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "#plt.xticks(epochs)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{results_folder}/losses_s{config['parameters']['subject']}_{config['parameters']['action']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Plot the losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, training_loss, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(epochs, validation_loss, label=\"Validation Loss\", marker='o')\n",
    "    plt.plot(epochs, test_losses, label=\"Test Loss\", linestyle='--', color='red')\n",
    "\n",
    "    # Add labels, title, legend\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training, Validation, and Test Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f\"{results_folder}/test_loss_s{config['parameters']['subject']}_{config['parameters']['action']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.randint(0, len(ecg_predictions_original_scale))\n",
    "ppg_scaling_factor = config['parameters']['ppg_scaling_factor']\n",
    "\n",
    "# Select the corresponding actual and predicted ECG signals\n",
    "ecg_predictions_random = ecg_predictions_original_scale[random_index]  # Predicted ECG signal\n",
    "ecg_actuals_random = ecg_actuals_original_scale[random_index]  # Actual ECG signal\n",
    "\n",
    "# Set the opacity value of alpha for the ppg signals\n",
    "alpha = 0.3\n",
    "\n",
    "# Plot the actual and predicted ECG\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ecg_actuals_random, label='Actual ECG')\n",
    "plt.plot(ecg_predictions_random, label='Predicted ECG')\n",
    "plt.title(f\"ECG Prediction vs Actual (Sequence {random_index})\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{results_folder}/random_seq_s{config['parameters']['subject']}_{config['parameters']['action']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual and predicted ECG with the input ppg signals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ecg_actuals_random, label='Actual ECG')\n",
    "plt.plot(ecg_predictions_random, label='Predicted ECG')\n",
    "plt.plot(ppg_scaling_factor*red_ppg[random_index], label=\"Red PPG\", alpha=alpha)\n",
    "plt.plot(ppg_scaling_factor*ir_ppg[random_index], label=\"IR PPG\", alpha=alpha)\n",
    "plt.plot(ppg_scaling_factor*green_ppg[random_index], label=\"Green PPG\", alpha=alpha)\n",
    "plt.title(f\"ECG Prediction vs Actual (Sequence {random_index}) with PPG signals\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{results_folder}/random_seq_s{config['parameters']['subject']}_{config['parameters']['action']}_ppg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random sequences to plot\n",
    "num_sequences = 1\n",
    "\n",
    "# Create a some more random sequences\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for _ in range(num_sequences):\n",
    "    random_index = np.random.randint(0, len(ecg_predictions_original_scale))\n",
    "    \n",
    "    # Select the corresponding actual and predicted ECG signals\n",
    "    ecg_predictions_random = ecg_predictions_original_scale[random_index]  # Predicted ECG signal\n",
    "    ecg_actuals_random = ecg_actuals_original_scale[random_index]  # Actual ECG signal\n",
    "\n",
    "    # Plot both actual and predicted ECG\n",
    "    plt.plot(ecg_actuals_random, label=f'Actual ECG {random_index}')\n",
    "    plt.plot(ecg_predictions_random, label=f'Predicted ECG {random_index}', linestyle='dashed')\n",
    "\n",
    "plt.title(\"ECG Predictions vs Actual for Random Sequences\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{results_folder}/mult_seq_s{config['parameters']['subject']}_{config['parameters']['action']}.png\")\n",
    "\n",
    "# Create a some more random sequences with the input ppg signals\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for _ in range(num_sequences):\n",
    "    random_index = np.random.randint(0, len(ecg_predictions_original_scale))\n",
    "    \n",
    "    # Select the corresponding actual and predicted ECG signals\n",
    "    ecg_predictions_random = ecg_predictions_original_scale[random_index]  # Predicted ECG signal\n",
    "    ecg_actuals_random = ecg_actuals_original_scale[random_index]  # Actual ECG signal\n",
    "\n",
    "    # Plot both actual and predicted ECG\n",
    "    plt.plot(ecg_actuals_random, label=f'Actual ECG {random_index}')\n",
    "    plt.plot(ecg_predictions_random, label=f'Predicted ECG {random_index}', linestyle='dashed')\n",
    "    plt.plot(ppg_scaling_factor*red_ppg[random_index], label=\"Red PPG\", alpha=0.25)\n",
    "    plt.plot(ppg_scaling_factor*ir_ppg[random_index], label=\"IR PPG\", alpha=0.25)\n",
    "    plt.plot(ppg_scaling_factor*green_ppg[random_index], label=\"Green PPG\", alpha=0.25)\n",
    "\n",
    "plt.title(\"ECG Predictions vs Actual for Random Sequences with PPG signals\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{results_folder}/mult_seq_s{config['parameters']['subject']}_{config['parameters']['action']}_ppg.png\")\n",
    "\n",
    "# Calculate the average ECG for both actual and predicted\n",
    "ecg_predictions_average = np.mean(ecg_actuals_original_scale, axis=0)  # Average over all predictions\n",
    "ecg_actuals_average = np.mean(ecg_predictions_original_scale, axis=0)  # Average over all sequences\n",
    "\n",
    "# Plot the average ECG\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ecg_actuals_average, label='Average Actual ECG')\n",
    "plt.plot(ecg_predictions_average, label='Average Predicted ECG')\n",
    "plt.title(\"Average ECG Prediction vs Actual\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{results_folder}/average_all_seq_s{config['parameters']['subject']}_{config['parameters']['action']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
