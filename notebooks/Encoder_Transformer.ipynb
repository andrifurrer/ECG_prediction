{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import psutil\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.profiler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Add the parent directory, i.e. transformer, means parent directory of 'scripts' and 'notebooks', to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import classes and functions\n",
    "from scripts.m1_functions import *\n",
    "from scripts.m1_classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_filtered = data_loader_filtered_single(subject=10, action='sit')\n",
    "print(df_filtered.shape)\n",
    "\n",
    "# Initialize scalers for predictors and target\n",
    "scaler_input = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform predictors (red ppg, ir ppg, green ppg)\n",
    "input_columns = ['red ppg', 'ir ppg', 'green ppg']\n",
    "x_normalized = scaler_input.fit_transform(df_filtered[input_columns])\n",
    "\n",
    "# Fit and transform target (ecg)\n",
    "y_normalized = scaler_target.fit_transform(df_filtered[['ecg']])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_data = torch.tensor(x_normalized, dtype=torch.float32) # Shape: [samples, 3] \n",
    "y_data = torch.tensor(y_normalized, dtype=torch.float32)  # Shape: [samples, 1] \n",
    "\n",
    "# Reshape for sequence input, adjust stepsize and subset\n",
    "sequence_length = 100\n",
    "sequence_step_size = 10\n",
    "num_sequences = len(df_filtered) - sequence_length + 1\n",
    "subset = 1\n",
    "\n",
    "x_sequences = torch.stack([x_data[i:i + sequence_length] for i in range(0, int(num_sequences*subset), int(sequence_step_size))])  # [num_sequences, seq_length, 3]\n",
    "y_sequences = torch.stack([y_data[i:i + sequence_length] for i in range(0, int(num_sequences*subset), int(sequence_step_size))])  # [num_sequences, seq_length, 1]\n",
    "\n",
    "# Split ratio \n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * x_sequences.size(0))  # Number of training samples\n",
    "val_size = x_sequences.size(0) - train_size          # Number of validation samples\n",
    "\n",
    "# Slicing of the ratio\n",
    "X_train, X_val = x_sequences[:train_size], x_sequences[train_size:]\n",
    "y_train, y_val = y_sequences[:train_size], y_sequences[train_size:]\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "# Model initialization \n",
    "d_model = 48  # Embedding dimension\n",
    "input_dim = 3  # 3 PPG signals (red, green, IR)\n",
    "output_dim = 1  # 1 ECG target per time step\n",
    "nhead = 6  # Attention heads\n",
    "num_layers = 4  # Number of transformer layers\n",
    "batch_size = 16  # Batch size\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = EncoderTransformerTimeSeries(input_dim=input_dim, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers).to(device) \n",
    " \n",
    "# Loss function: Mean Squared Error for regression tasks\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "\n",
    "# Clear any residual memory before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "### Training\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0 # Initialize running loss\n",
    "\n",
    "    # Iterate through the training data in batches\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # Get the current batch\n",
    "        batch_X = X_train[i:i+batch_size].to(device)\n",
    "        batch_y = y_train[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        predictions = model(batch_X)\n",
    "\n",
    "        # Calculate loss (MSE between predicted ECG and actual ECG)\n",
    "        loss = loss_fn(predictions, batch_y)\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    avg_train_loss = running_loss / len(X_train)\n",
    "    train_rmse = torch.sqrt(torch.tensor(avg_train_loss)) # MSE needs to be calculated at the end of each batch, scaled by batch size and the RMSE should calculated at the end of the epoch (metric)\n",
    "    \n",
    "    # Validation metrics with batching\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(X_val), batch_size):\n",
    "            # Get the current validation batch\n",
    "            batch_X_val = X_val[j:j + batch_size].to(device)\n",
    "            batch_y_val = y_val[j:j + batch_size].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            val_predictions = model(batch_X_val)\n",
    "\n",
    "            # Calculate loss for this batch\n",
    "            val_loss = loss_fn(val_predictions, batch_y_val)\n",
    "\n",
    "            # Accumulate total validation loss\n",
    "            total_val_loss += val_loss.item() * batch_X_val.size(0)  # Weighted by batch size, necessary and if so why not for X_batch?\n",
    "\n",
    "    # Average validation loss over all samples\n",
    "    avg_val_loss = total_val_loss / len(X_val) \n",
    "    val_rmse = torch.sqrt(torch.tensor(avg_val_loss)) # MSE needs to be calculated at the end of each batch, scaled by batch size and the RMSE should calculated at the end of the epoch (metric)\n",
    "\n",
    "    # Clear any residual memory before start of new epoch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train RMSE Loss: {train_rmse:.4f} | Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '../models/enoder_m2_ecg_model_all_data.pth')\n",
    "\n",
    "\n",
    "### Validation\n",
    "# Initialize storage for aggregated predictions and actual values\n",
    "ecg_predictions = []\n",
    "ecg_actuals = []\n",
    "ppg = []\n",
    "\n",
    "# Iterate over the validation set in batches\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "with torch.no_grad():\n",
    "    for j in range(0, len(X_val), batch_size):\n",
    "        # Get the current validation batch\n",
    "        batch_X_val = X_val[j:j + batch_size].to(device)\n",
    "        batch_y_val = y_val[j:j + batch_size].to(device)\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        batch_predictions = model(batch_X_val)\n",
    "\n",
    "        # Store predictions and actuals\n",
    "        ecg_predictions.append(batch_predictions.cpu())  # Move to CPU for numpy/scaler operations\n",
    "        ecg_actuals.append(batch_y_val.cpu())\n",
    "        ppg.append(batch_X_val.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "ecg_predictions = torch.cat(ecg_predictions, dim=0)\n",
    "ecg_actuals = torch.cat(ecg_actuals, dim=0)\n",
    "ppg = torch.cat(ppg, dim=0)\n",
    "\n",
    "# Reverse transform predicted ECG to original scale\n",
    "ecg_predictions = ecg_predictions.squeeze(-1)\n",
    "ecg_predictions_original_scale = scaler_target.inverse_transform(ecg_predictions.numpy())\n",
    "\n",
    "# Reverse transform actual ECG to original scale\n",
    "ecg_actuals = ecg_actuals.squeeze(-1)\n",
    "ecg_actuals_original_scale = scaler_target.inverse_transform(ecg_actuals.numpy())\n",
    "\n",
    "# Reverse transform ppg to orignial scale\n",
    "ppg = ppg.squeeze(-1)\n",
    "#ppg_original_scale = scaler_input.inverse_transform(ppg[input_columns].numpy())\n",
    "\n",
    "\n",
    "### Evaluation metrics\n",
    "# Predictions and actual values (already scaled back to original scale)\n",
    "ecg_predictions_original_scale_flattened = ecg_predictions_original_scale.flatten()  # Flatten to 1D if necessary\n",
    "ecg_actuals_original_scale_flattened = ecg_actuals_original_scale.flatten()\n",
    "\n",
    "# Calculate the range of the actual data for normalization\n",
    "actual_range = np.ptp(ecg_actuals_original_scale)  # Peak-to-peak (max - min)\n",
    "\n",
    "# Euclidean Distance\n",
    "euclidean_distance = euclidean(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened)\n",
    "\n",
    "# Dynamic Time Warping (DTW)\n",
    "downsampling_factor = 10\n",
    "batch_size = 10 \n",
    "dtw_distance = compute_batched_dtw(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened, batch_size, downsampling_factor)\n",
    "# dtw_distance = alignment.distance\n",
    "\n",
    "# Pearson Correlation\n",
    "pearson_corr, _ = pearsonr(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened)\n",
    "\n",
    "# Spearman Correlation\n",
    "spearman_corr, _ = spearmanr(ecg_predictions_original_scale_flattened, ecg_actuals_original_scale_flattened)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = np.mean((ecg_predictions_original_scale_flattened - ecg_actuals_original_scale_flattened) ** 2)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = np.mean(np.abs(ecg_predictions_original_scale_flattened - ecg_actuals_original_scale_flattened))\n",
    "\n",
    "# Normalized Root Mean Squared Error (NRMSE)\n",
    "nrmse = rmse / actual_range\n",
    "\n",
    "# Normalized Mean Absolute Error (NMAE)\n",
    "nmae = mae / actual_range\n",
    "\n",
    "# Print metrics\n",
    "metrics = {\n",
    "    \"Euclidean Distance\": euclidean_distance,\n",
    "    \"DTW Distance\": dtw_distance,\n",
    "    \"Pearson Correlation\": pearson_corr,\n",
    "    \"Spearman Correlation\": spearman_corr,\n",
    "    \"MSE\": mse,\n",
    "    \"RMSE\": rmse,\n",
    "    \"MAE\": mae,\n",
    "    \"NRMSE\": nrmse,\n",
    "    \"NMAE\": nmae,\n",
    "}\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating graphs\n",
    "# Randomly select an index from the validation data\n",
    "random_index = np.random.randint(0, len(ecg_predictions_original_scale))\n",
    "\n",
    "# Select the corresponding actual and predicted ECG signals\n",
    "ecg_predictions_random = ecg_predictions_original_scale[random_index]  # Predicted ECG signal\n",
    "ecg_actuals_random = ecg_actuals_original_scale[random_index]  # Actual ECG signal\n",
    "#ppg_random = ppg_original_scale[random_index] # PPG Signal\n",
    "\n",
    "# Plot the actual and predicted ECG\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ecg_actuals_random, label='Actual ECG')\n",
    "plt.plot(ecg_predictions_random, label='Predicted ECG')\n",
    "plt.plot(ppg_random, label='PPG signal')\n",
    "plt.title(f\"ECG Prediction vs Actual (Sequence {random_index})\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = data_loader_filtered()\n",
    "df_original = data_loader_original()\n",
    "df_filtered_single = data_loader_filtered_single(subject=10, action='sit')\n",
    "df_original_single = data_loader_original_single(subject=10, action='sit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subjects and action you want to filter\n",
    "selected_subjects = [1,2,3,4,5,6]  # Replace with desired subject IDs\n",
    "selected_action = 'sit'    # Replace with the desired action\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_custom = df_filtered[(df_filtered['subject'].isin(selected_subjects)) & (df_filtered['action'] == selected_action)]\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "df_custom = df_custom.reset_index(drop=True)\n",
    "\n",
    "# Verify the result\n",
    "print(df_custom.head())\n",
    "print(f\"New DataFrame shape: {df_custom.shape}\")\n",
    "df_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 253000\n",
    "stop = 255000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time points\n",
    "time = df_custom.index  # Assuming your dataframe has a time-based index\n",
    "\n",
    "# Plot the normalized ECG time-series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time[start:stop], df_custom['ecg'][start:stop], label=\"Normalized ECG Signal\", color='blue')\n",
    "plt.title(\"Normalized ECG Time-Series\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot all PPG signals and ECG on the same graph\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time[start:stop], df_custom['red ppg'][start:stop], label=\" Red PPG\", color='red')\n",
    "plt.plot(time[start:stop], df_custom['green ppg'][start:stop], label=\" Green PPG\", color='green')\n",
    "plt.plot(time[start:stop], df_custom['ir ppg'][start:stop], label=\" IR PPG\", color='purple')\n",
    "plt.title(\" PPG Signals\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\" Value\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot ECG signal\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time[start:stop], df_custom['ecg'][start:stop], label=\" ECG\", color='blue')\n",
    "plt.title(\" ECG Signal\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\" Value\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom_normalized, scalers = normalization_group_action(df_custom)\n",
    "df_custom_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot all PPG signals and ECG on the same graph\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time[start:stop], df_custom_normalized['red ppg'][start:stop], label=\"Normalized Red PPG\", color='red')\n",
    "plt.plot(time[start:stop], df_custom_normalized['green ppg'][start:stop], label=\"Normalized Green PPG\", color='green')\n",
    "plt.plot(time[start:stop], df_custom_normalized['ir ppg'][start:stop], label=\"Normalized IR PPG\", color='purple')\n",
    "plt.title(\"Normalized PPG Signals\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot ECG signal\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time[start:stop], df_custom_normalized['ecg'][start:stop], label=\"Normalized ECG\", color='blue')\n",
    "plt.title(\"Normalized ECG Signal\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time points\n",
    "time = df_custom_normalized.index  # Assuming your dataframe has a time-based index\n",
    "\n",
    "# Plot the normalized ECG time-series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time[253000:255000], df_custom_normalized['ecg'][253000:255000], label=\"Normalized ECG Signal\", color='blue')\n",
    "plt.title(\"Normalized ECG Time-Series\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratios for train, validation, and test splits\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "sequence_length = 1000\n",
    "sequence_step_size = 100\n",
    "subset = 1\n",
    "\n",
    "# Generate sequences\n",
    "x_data, y_data = sequences(df_custom_normalized, sequence_length, sequence_step_size, subset)\n",
    "\n",
    "# Calculate sizes for each subset\n",
    "total_samples = x_data.size(0)\n",
    "train_size = int(train_ratio * total_samples)\n",
    "val_size = int(val_ratio * total_samples)\n",
    "test_size = total_samples - train_size - val_size  # Remaining samples go to the test set\n",
    "\n",
    "# Split the data\n",
    "X_train = x_data[:train_size]\n",
    "y_train = y_data[:train_size]\n",
    "\n",
    "X_val = x_data[train_size:train_size + val_size]\n",
    "y_val = y_data[train_size:train_size + val_size]\n",
    "\n",
    "X_test = x_data[train_size + val_size:]\n",
    "y_test = y_data[train_size + val_size:]\n",
    "\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"x_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"x_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization \n",
    "d_model = 64  # Embedding dimension\n",
    "input_dim = 3  # 3 PPG signals (red, green, IR)\n",
    "output_dim = 1  # 1 ECG target per time step\n",
    "nhead = 2  # Attention heads\n",
    "num_layers = 2  # Number of transformer layers\n",
    "batch_size = 8  # Batch size\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Convert tensors to Datasets\n",
    "train_dataset = PreprocessedDataset(X_train, y_train)\n",
    "val_dataset = PreprocessedDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders with a reproducible generator\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(seed)\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, generator=gen)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = EncoderTransformerTimeSeries(input_dim=input_dim, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers).to(device) \n",
    "\n",
    "X_train_sample = X_train[:1]\n",
    "y_train_sample = y_train[:1]\n",
    "\n",
    "# Call the torchinfo summary method\n",
    "summary_txt = summary(model, input_data=X_train_sample, depth=1, device=device)\n",
    "print(summary_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for batch_X, batch_y in train_loader:\n",
    "    batch_X = batch_X.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "    c +=1\n",
    "    \n",
    "print(batch_X.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Mean Squared Error for regression tasks\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Adam optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)\n",
    "\n",
    "# Number of epochs to train\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize a learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Clear any residual memory before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Arrays for storing losses and epochs\n",
    "training_loss = np.array([])\n",
    "validation_loss = np.array([])\n",
    "epochs = np.array([])\n",
    "best_models = np.array([])\n",
    "\n",
    "# Early stopping and checkpoint parameters\n",
    "patience = 10\n",
    "min_delta = 1e-4\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "\n",
    "### Training\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0 # Initialize running loss\n",
    "    # Iterate through the batches in the train_loader to load the data in batches\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        predictions = model(batch_X)\n",
    "\n",
    "        # Calculate loss (MSE between predicted ECG and actual ECG)\n",
    "        loss = loss_fn(predictions, batch_y)\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    avg_train_loss = running_loss / len(X_train)\n",
    "    train_rmse = torch.sqrt(torch.tensor(avg_train_loss)) # MSE needs to be calculated at the end of each batch, scaled by batch size and the RMSE should calculated at the end of the epoch (metric)\n",
    "    training_loss = np.append(training_loss, train_rmse.cpu())\n",
    "\n",
    "    print(f\"Training of epoch {epoch+1} done, starting validation!\")\n",
    "    # Validation metrics with batching\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate through the batches in the val_loader to load the data in batches\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            batch_X_val = batch_X_val.to(device)\n",
    "            batch_y_val = batch_y_val.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            val_predictions = model(batch_X_val)\n",
    "\n",
    "            # Calculate loss for this batch\n",
    "            val_loss = loss_fn(val_predictions, batch_y_val)\n",
    "\n",
    "            # Accumulate total validation loss\n",
    "            total_val_loss += val_loss.item() * batch_X_val.size(0)  # Weighted by batch size\n",
    "\n",
    "\n",
    "        # Average validation loss over all samples\n",
    "        avg_val_loss = total_val_loss / len(X_val) \n",
    "        val_rmse = torch.sqrt(torch.tensor(avg_val_loss)) # MSE needs to be calculated at the end of each batch, scaled by batch size and the RMSE should calculated at the end of the epoch (metric)\n",
    "        validation_loss = np.append(validation_loss, val_rmse.cpu())\n",
    "\n",
    "        # Step the learning rate scheduler with the validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_counter = 0\n",
    "            # Save checkpoint\n",
    "            #checkpoint_path = f\"{checkpoints_folder}/epoch{epoch+1}.pth\"\n",
    "            #save_checkpoint(model, optimizer, epoch + 1, avg_val_loss, checkpoint_path)\n",
    "\n",
    "            # Save the model\n",
    "            #torch.save(model.state_dict(), f\"../models/{model_family}{model_name}_trained_model_epoch{epoch+1}.pth\")\n",
    "\n",
    "            # Save the epoch of this best model\n",
    "            best_models = np.append(best_models, int(epoch+1))\n",
    "\n",
    "            epochs = np.append(epochs, int(epoch+1))\n",
    "            print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n",
    "        else:\n",
    "            epochs = np.append(epochs, int(epoch+1))\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "        #print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train RMSE: {train_rmse:.4f} | Val RMSE: {val_rmse:.4f} | Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Clear any residual memory before start of new epoch\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation\n",
    "# Initialize storage for aggregated predictions and actual values\n",
    "ecg_predictions = []\n",
    "ecg_actuals = []\n",
    "ppg = []\n",
    "subjects = []  # Store subject info for each batch\n",
    "actions = []   # Store action info for each batch\n",
    "\n",
    "# Loss function: Mean Squared Error for regression tasks\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Test Loss\n",
    "test_loss = np.array([])\n",
    "running_test_loss = 0\n",
    "\n",
    "# Convert tensors to Datasets\n",
    "test_dataset = PreprocessedDataset(X_test, y_test)\n",
    "# Create DataLoaders for each set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Iterate over the validation set in batches\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Iterate through the batches in the test_loader to load the data in batches\n",
    "    for batch_idx, (batch_X_test, batch_y_test) in enumerate(test_loader):\n",
    "        # Move the batch data to the device (GPU or CPU)\n",
    "        batch_X_test = batch_X_test.to(device)\n",
    "        batch_y_test = batch_y_test.to(device)\n",
    "        \n",
    "        # Get the start and end index of the current batch in df_test\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + len(batch_X_test)\n",
    "        \n",
    "        # Retrieve the corresponding (subject, action) pair for this batch from df_test\n",
    "        batch_subjects = df_test.iloc[start_idx:end_idx]['subject'].values\n",
    "        batch_actions = df_test.iloc[start_idx:end_idx]['action'].values\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        batch_predictions = model(batch_X_test)\n",
    "\n",
    "        # Calculate loss for this batch\n",
    "        loss = loss_fn(batch_predictions, batch_y_test)\n",
    "\n",
    "        # Accumulate total validation loss\n",
    "        running_test_loss += loss.item() * batch_X_test.size(0)\n",
    "\n",
    "        # Store predictions, actuals, subjects, and actions\n",
    "        ecg_predictions.append(batch_predictions.cpu())  # Move to CPU for numpy/scaler operations\n",
    "        ecg_actuals.append(batch_y_test.cpu())\n",
    "        ppg.append(batch_X_test.cpu())\n",
    "        subjects.extend(batch_subjects)\n",
    "        actions.extend(batch_actions)\n",
    "\n",
    "# Average the test loss over all samples\n",
    "avg_test_loss = running_test_loss / len(X_test)\n",
    "test_rmse = torch.sqrt(torch.tensor(avg_test_loss))\n",
    "test_loss = np.append(test_loss, test_rmse.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "ecg_predictions = torch.cat(ecg_predictions, dim=0)\n",
    "ecg_actuals = torch.cat(ecg_actuals, dim=0)\n",
    "ppg = torch.cat(ppg, dim=0)\n",
    "\n",
    "# Initialize lists for original scale data\n",
    "ecg_predictions_original_scale = []\n",
    "ecg_actuals_original_scale = []\n",
    "ppg_original_scale = []\n",
    "\n",
    "# Process each sequence\n",
    "for i in range(len(ecg_predictions)):\n",
    "    # Get subject and action for the current sequence\n",
    "    subject = subjects[i]\n",
    "    action = actions[i]\n",
    "\n",
    "    # Retrieve the correct scalers\n",
    "    scaler_input = scalers[(subject, action)]['input_scaler']\n",
    "    scaler_target = scalers[(subject, action)]['target_scaler']\n",
    "\n",
    "    # Inverse transform predictions and actuals for the current sequence\n",
    "    ecg_pred = ecg_predictions[i].squeeze(-1).numpy()  # Shape: [sequence_length]\n",
    "    ecg_act = ecg_actuals[i].squeeze(-1).numpy()       # Shape: [sequence_length]\n",
    "    ppg_seq = ppg[i].numpy()                          # Shape: [sequence_length, 3]\n",
    "\n",
    "    ecg_predictions_original_scale.append(scaler_target.inverse_transform(ecg_pred.reshape(-1, 1)).flatten())\n",
    "    ecg_actuals_original_scale.append(scaler_target.inverse_transform(ecg_act.reshape(-1, 1)).flatten())\n",
    "    ppg_original_scale.append(scaler_input.inverse_transform(ppg_seq))\n",
    "\n",
    "# Convert back to arrays\n",
    "ecg_predictions_original_scale = np.array(ecg_predictions_original_scale)\n",
    "ecg_actuals_original_scale = np.array(ecg_actuals_original_scale)\n",
    "ppg_original_scale = np.array(ppg_original_scale)\n",
    "\n",
    "# Separate PPG channels \n",
    "red_ppg = ppg_original_scale[:, :, 0]  # Red PPG\n",
    "ir_ppg = ppg_original_scale[:, :, 1]   # IR PPG\n",
    "green_ppg = ppg_original_scale[:, :, 2]  # Green PPG\n",
    "\n",
    "\n",
    "### Normalized Evaluation metrics\n",
    "# Predictions and actual values (normalized and flattened)\n",
    "ecg_predictions_arr = np.array(ecg_predictions).flatten()\n",
    "ecg_actuals_arr = np.array(ecg_actuals).flatten()\n",
    "\n",
    "# Calculate the range of the actual data for normalization\n",
    "actual_range_normalized = np.ptp(ecg_actuals)  # Peak-to-peak (max - min)\n",
    "\n",
    "# Euclidean Distance\n",
    "euclidean_distance_normalized = euclidean(ecg_predictions_arr, ecg_actuals_arr)\n",
    "\n",
    "# Dynamic Time Warping (DTW)\n",
    "downsampling_factor_dtw = 10\n",
    "batch_size_dtw = 10 \n",
    "dtw_distance_normalized = compute_batched_dtw(ecg_predictions_arr, ecg_actuals_arr, batch_size_dtw, downsampling_factor_dtw)\n",
    "# dtw_distance = alignment.distance\n",
    "\n",
    "# Pearson Correlation\n",
    "pearson_corr_normalized, _ = pearsonr(ecg_predictions_arr, ecg_actuals_arr)\n",
    "\n",
    "# Spearman Correlation\n",
    "spearman_corr_normalized, _ = spearmanr(ecg_predictions_arr, ecg_actuals_arr)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_normalized = np.mean((ecg_predictions_arr - ecg_actuals_arr) ** 2)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae_normalized = np.mean(np.abs(ecg_predictions_arr - ecg_actuals_arr))\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse_normalized = np.sqrt(mse_normalized)\n",
    "\n",
    "# Normalized Root Mean Squared Error (NRMSE)\n",
    "nrmse_normalized = rmse_normalized / actual_range_normalized\n",
    "\n",
    "# Normalized Mean Absolute Error (NMAE)\n",
    "nmae_normalized = mae_normalized / actual_range_normalized\n",
    "\n",
    "# Print metrics\n",
    "metrics_normalized = {\n",
    "    \"Training_loss\": training_loss,\n",
    "    \"Validation_loss\": validation_loss,\n",
    "    \"Test_loss\": test_loss,\n",
    "    \"Epochs\": epochs, \n",
    "    \"Euclidean Distance\": euclidean_distance_normalized,\n",
    "    \"DTW Distance\": dtw_distance_normalized,\n",
    "    \"Pearson Correlation\": pearson_corr_normalized,\n",
    "    \"Spearman Correlation\": spearman_corr_normalized,\n",
    "    \"MSE\": mse_normalized,\n",
    "    \"MAE\": mae_normalized,\n",
    "    \"RMSE\": rmse_normalized,\n",
    "    \"NRMSE\": nrmse_normalized,\n",
    "    \"NMAE\": nmae_normalized,\n",
    "    # \"Parameters\": config['parameters'],  # Add config file entries\n",
    "    # \"General\": config['general'],\n",
    "    # \"Output\": config['output'],\n",
    "}\n",
    "\n",
    "for metric, value in metrics_normalized.items():\n",
    "    #print(f\"{metric}: {value:.4f}\")\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_predictions_original_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots\n",
    "# Call a plot(...) method to create them\n",
    "# Randomly select an index from the validation data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_loss,  label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.title(f\"Training and Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "#plt.xticks(epochs)\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(f\"{results_folder}/{model_name}_loss_functions.png\")\n",
    "\n",
    "    # Repeat test loss across all epochs for visualization\n",
    "test_losses = [test_loss] * len(epochs)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_loss, label=\"Training Loss\", marker='o')\n",
    "plt.plot(epochs, validation_loss, label=\"Validation Loss\", marker='o')\n",
    "plt.plot(epochs, test_losses, label=\"Test Loss\", linestyle='--', color='red')\n",
    "\n",
    "# Add labels, title, legend\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training, Validation, and Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(f\"{results_folder}/{model_name}_test_loss.png\")\n",
    "\n",
    "#random_index = np.random.randint(0, len(ecg_predictions_original_scale))\n",
    "random_index = 1\n",
    "ppg_scaling_factor = 1\n",
    "\n",
    "# Select the corresponding actual and predicted ECG signals\n",
    "ecg_predictions_random = ecg_predictions_original_scale[random_index]  # Predicted ECG signal\n",
    "ecg_actuals_random = ecg_actuals_original_scale[random_index]  # Actual ECG signal\n",
    "\n",
    "# Set the opacity value of alpha for the ppg signals\n",
    "alpha = 0.3\n",
    "\n",
    "# Plot the actual and predicted ECG\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ecg_actuals_random, label='Actual ECG')\n",
    "plt.plot(ecg_predictions_random, label='Predicted ECG')\n",
    "plt.title(f\"ECG Prediction vs Actual (Sequence {random_index})\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(f\"{results_folder}/{model_name}_random_seq.png\")\n",
    "\n",
    "# Plot the actual and predicted ECG with the input ppg signals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ecg_actuals_random, label='Actual ECG')\n",
    "plt.plot(ecg_predictions_random, label='Predicted ECG')\n",
    "plt.plot(ppg_scaling_factor*red_ppg[random_index], label=\"Red PPG\", alpha=alpha)\n",
    "plt.plot(ppg_scaling_factor*ir_ppg[random_index], label=\"IR PPG\", alpha=alpha)\n",
    "plt.plot(ppg_scaling_factor*green_ppg[random_index], label=\"Green PPG\", alpha=alpha)\n",
    "plt.title(f\"ECG Prediction vs Actual (Sequence {random_index}) with PPG signals\")\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('ECG Signal')\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(f\"{results_folder}/{model_name}_random_seq_ppg.png\")\n",
    "\n",
    "print(\"Evaluation finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
